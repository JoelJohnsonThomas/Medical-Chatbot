# Medical Chatbot ğŸ¥

An intelligent medical chatbot powered by Google's Gemini AI, LangChain, and Pinecone vector database. This application provides accurate medical information by leveraging Retrieval-Augmented Generation (RAG) to answer health-related questions based on medical documents.

## ğŸŒŸ Features

- **AI-Powered Responses**: Uses Google Gemini 2.5 Pro for intelligent medical question answering
- **RAG Architecture**: Retrieval-Augmented Generation ensures accurate, context-aware responses
- **Vector Search**: Pinecone vector database for efficient similarity search
- **Modern UI**: Beautiful, responsive chat interface with animations and glassmorphism effects
- **Real-time Interaction**: Instant responses with typing indicators and smooth animations
- **Document Processing**: Automatically processes PDF medical documents for knowledge base

## ğŸ“‹ Prerequisites

- Python 3.10 or higher
- Conda (Anaconda or Miniconda)
- Google API Key (for Gemini AI)
- Pinecone API Key (for vector database)

## ğŸš€ Installation

### Step 1: Clone the Repository
```bash
git clone https://github.com/JoelJohnsonThomas/Medical-Chatbot.git
cd Medical-Chatbot
```

### Step 2: Create a Conda Environment
Create a new Conda environment named `Medical-Chatbot` with Python 3.10:
```bash
conda create -n Medical-Chatbot python=3.10 -y
```

### Step 3: Activate the Environment
Activate the newly created environment:
```bash
conda activate Medical-Chatbot
```

### Step 4: Install Dependencies
Install the required Python packages:
```bash
pip install -r requirements.txt
```

## âš™ï¸ Configuration

### Step 1: Set Up Environment Variables
Create a `.env` file in the root directory with your API keys:
```env
PINECONE_API_KEY=your_pinecone_api_key_here
GOOGLE_API_KEY=your_google_api_key_here
```

**How to get API keys:**
- **Google API Key**: Visit [Google AI Studio](https://makersuite.google.com/app/apikey) to create your API key
- **Pinecone API Key**: Sign up at [Pinecone](https://www.pinecone.io/) and get your API key from the dashboard

### Step 2: Prepare Medical Documents
Place your medical PDF documents in the `data/` directory. These documents will be used to build the knowledge base.

## ğŸ“Š Data Indexing

Before running the chatbot, you need to create the vector index from your medical documents:

```bash
python store_index.py
```

This script will:
1. Load PDF documents from the `data/` directory
2. Split documents into manageable chunks
3. Generate embeddings using HuggingFace's sentence-transformers
4. Create/update the Pinecone vector index named "medical-chatbot"

**Note**: This step only needs to be run once, or when you add new documents to the knowledge base.

## ğŸ¯ Running the Application

### Start the Flask Server
```bash
python app.py
```

The application will start on `http://localhost:5000`

### Access the Chatbot
Open your web browser and navigate to:
```
http://localhost:5000
```

You can now start asking medical questions!

## ğŸ“ Project Structure

```
Medical-Chatbot/
â”œâ”€â”€ app.py                      # Flask application and API endpoints
â”œâ”€â”€ store_index.py              # Script to create Pinecone vector index
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env                        # Environment variables (API keys)
â”œâ”€â”€ data/                       # Medical PDF documents
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ helper.py              # Helper functions for PDF loading and processing
â”‚   â””â”€â”€ prompt.py              # System prompts for the AI model
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ chat.html              # Chat interface HTML
â””â”€â”€ static/
    â””â”€â”€ style.css              # Styling for the chat interface
```

## ğŸ› ï¸ Technology Stack

- **Backend Framework**: Flask 3.1.1
- **AI Model**: Google Gemini 2.5 Pro
- **LLM Framework**: LangChain 0.3.26
- **Vector Database**: Pinecone
- **Embeddings**: HuggingFace sentence-transformers (all-MiniLM-L6-v2)
- **Document Processing**: PyPDF 5.6.1
- **Frontend**: HTML, CSS, JavaScript with modern animations

## ğŸ”§ Key Components

### RAG Pipeline
1. **Document Loading**: PDFs are loaded from the `data/` directory
2. **Text Splitting**: Documents are split into 500-character chunks with 20-character overlap
3. **Embedding**: Text chunks are converted to 384-dimensional vectors
4. **Vector Storage**: Embeddings are stored in Pinecone for similarity search
5. **Retrieval**: Top 3 most relevant chunks are retrieved for each query
6. **Generation**: Gemini AI generates responses based on retrieved context

### API Endpoints
- `GET /`: Renders the chat interface
- `POST /get`: Processes user messages and returns AI responses

## ğŸ’¡ Usage Tips

- Ask specific medical questions for best results
- The chatbot uses a concise response format (max 3 sentences)
- If the chatbot doesn't know an answer, it will honestly say so
- Responses are based on the medical documents in your knowledge base

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

**Joel Johnson Thomas**
- GitHub: [@JoelJohnsonThomas](https://github.com/JoelJohnsonThomas)

## ğŸ™ Acknowledgments

- Google Gemini AI for powerful language understanding
- LangChain for the RAG framework
- Pinecone for vector database infrastructure
- HuggingFace for embedding models